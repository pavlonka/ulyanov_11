# -*- coding: utf-8 -*-
"""ulysses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gY14ry_PKs7EJ50pHv20gqWH8Tjz0b4S
"""

import requests
from bs4 import BeautifulSoup
from collections import Counter
import re

def load_chapter_text(url, chapter_id='chap01'):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    start_tag = soup.find(id=chapter_id)
    if not start_tag:
        raise ValueError(f"Chapter id '{chapter_id}' not found")

    texts = []
    for sibling in start_tag.find_next_siblings():
        if sibling.name and sibling.name.startswith('h'):
            break
        texts.append(sibling.get_text(separator=' ', strip=True))
    chapter_text = ' '.join(texts)
    return chapter_text

url = 'https://www.gutenberg.org/files/4300/4300-h/4300-h.htm#chap01'
chapter_text = load_chapter_text(url)

words = re.findall(r'\b\w+\b', chapter_text.lower())
word_freq = Counter(words)

print(f"Всего уникальных слов в 1-й главе: {len(word_freq)}")

def find_word_context(text, word, left_context=5, right_context=5, cut_length=False, output_file='contexts.txt'):
    """
    text: строка с текстом
    word: искомое слово (без учёта регистра)
    left_context: количество слов слева
    right_context: количество слов справа
    cut_length: если True, контекст ограничивается рамками предложения
    output_file: имя файла для записи результатов
    """
    word = word.lower()

    sentences = re.split(r'(?<=[.!?])\s+', text)


    sentences_words = [re.findall(r'\b\w+\b', sent.lower()) for sent in sentences]

    results = []

    for sent_words in sentences_words:
        indices = [i for i, w in enumerate(sent_words) if w == word]
        for idx in indices:

            if cut_length:
                left_start = max(0, idx - left_context)
                right_end = min(len(sent_words), idx + right_context + 1)
            else:

                all_words = re.findall(r'\b\w+\b', text.lower())

                positions = [i for i, w in enumerate(all_words) if w == word]

                pos = positions[0]
                left_start = max(0, pos - left_context)
                right_end = min(len(all_words), pos + right_context + 1)
                context_words = all_words[left_start:pos] + [word] + all_words[pos+1:right_end]

                context_str = ' '.join(context_words)
                results.append(context_str)
                continue

            context_words = sent_words[left_start:idx] + [word] + sent_words[idx+1:right_end]
            context_str = ' '.join(context_words)
            results.append(context_str)


    with open(output_file, 'w', encoding='utf-8') as f:
        for line in results:
            print(line)
            f.write(line + '\n')

find_word_context(chapter_text, word='ulysses', left_context=5, right_context=5, cut_length=True)